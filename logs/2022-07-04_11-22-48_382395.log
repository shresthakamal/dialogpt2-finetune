2022-07-04 at 11:22:48 
{'batch': 4, 'epochs': 3, 'save': 'dialogpt-finetune', 'lr': 5e-05, 'clip': 2.0, 'seed': 1234, 'context': 3, 'max_len': 128, 'prepare': False, 'grad_accumulate': 64, 'tensorboard': 'runs/', 'early_stop': 10}
2022-07-04 at 11:22:48 Working on GPU: cuda
2022-07-04 at 11:22:48 Loading Tokenizer ...
2022-07-04 at 11:22:58 Loading Saved Dataset ...
2022-07-04 at 11:23:28 Loading the model ...
2022-07-04 at 11:23:39 Training the model ...

2022-07-04 at 11:23:39 Epoch: 0, Batch: 0, Loss: 0.3491613566875458
2022-07-04 at 11:24:18 Epoch: 0, Batch: 100, Loss: 0.27892959117889404
2022-07-04 at 11:24:56 Epoch: 0, Batch: 200, Loss: 0.18419472873210907
2022-07-04 at 11:25:35 Epoch: 0, Batch: 300, Loss: 0.1461428701877594
2022-07-04 at 11:26:13 Epoch: 0, Batch: 400, Loss: 0.05217161774635315
2022-07-04 at 11:26:51 Epoch: 0, Batch: 500, Loss: 0.02642197161912918
2022-07-04 at 11:27:30 Epoch: 0, Batch: 600, Loss: 0.002325221197679639
2022-07-04 at 11:28:08 Epoch: 0, Batch: 700, Loss: 0.0007936239708214998
2022-07-04 at 11:28:47 Epoch: 0, Batch: 800, Loss: 3.402080210435088e-06
2022-07-04 at 11:29:25 Epoch: 0, Batch: 900, Loss: 2.167683987863711e-07
2022-07-04 at 11:30:04 Epoch: 0, Batch: 1000, Loss: 6.45918376562804e-08
2022-07-04 at 11:30:42 Epoch: 0, Batch: 1100, Loss: 9.073115592173053e-09
2022-07-04 at 11:31:20 Epoch: 0, Batch: 1200, Loss: 4.845785994689322e-09
2022-07-04 at 11:31:59 Epoch: 0, Batch: 1300, Loss: 1.2150848105463297e-09
2022-07-04 at 11:32:37 Epoch: 0, Batch: 1400, Loss: 2.6557242516211943e-10
2022-07-04 at 11:33:16 Epoch: 0, Batch: 1500, Loss: 7.275957180502557e-12
2022-07-04 at 11:33:54 Epoch: 0, Batch: 1600, Loss: 4.001776340856189e-11
2022-07-04 at 11:34:33 Epoch: 0, Batch: 1700, Loss: 7.275957180502557e-12
2022-07-04 at 11:35:11 Epoch: 0, Batch: 1800, Loss: 0.0
2022-07-04 at 11:35:50 Epoch: 0, Batch: 1900, Loss: 0.0
2022-07-04 at 11:36:28 Epoch: 0, Batch: 2000, Loss: 0.0
2022-07-04 at 11:37:06 Epoch: 0, Batch: 2100, Loss: 0.0
2022-07-04 at 11:37:45 Epoch: 0, Batch: 2200, Loss: 0.0
2022-07-04 at 11:38:23 Epoch: 0, Batch: 2300, Loss: 0.0
2022-07-04 at 11:39:02 Epoch: 0, Batch: 2400, Loss: 0.0
2022-07-04 at 11:39:02 10 consecutive minimal loss, Epoch Early Skipped.
2022-07-04 at 11:39:07 Epoch: 1, Batch: 0, Loss: 0.0
2022-07-04 at 11:39:46 Epoch: 1, Batch: 100, Loss: 0.0
2022-07-04 at 11:40:24 Epoch: 1, Batch: 200, Loss: 0.0
2022-07-04 at 11:41:02 Epoch: 1, Batch: 300, Loss: 0.0
2022-07-04 at 11:41:41 Epoch: 1, Batch: 400, Loss: 0.0
2022-07-04 at 11:42:19 Epoch: 1, Batch: 500, Loss: 0.0
2022-07-04 at 11:42:58 Epoch: 1, Batch: 600, Loss: 0.0
2022-07-04 at 11:43:36 Epoch: 1, Batch: 700, Loss: 0.0
2022-07-04 at 11:44:15 Epoch: 1, Batch: 800, Loss: 0.0
2022-07-04 at 11:44:53 Epoch: 1, Batch: 900, Loss: 0.0
2022-07-04 at 11:44:53 10 consecutive minimal loss, Epoch Early Skipped.
2022-07-04 at 11:44:58 Epoch: 2, Batch: 0, Loss: 0.0
2022-07-04 at 11:45:37 Epoch: 2, Batch: 100, Loss: 0.0
2022-07-04 at 11:46:15 Epoch: 2, Batch: 200, Loss: 0.0
2022-07-04 at 11:46:54 Epoch: 2, Batch: 300, Loss: 0.0
2022-07-04 at 11:47:32 Epoch: 2, Batch: 400, Loss: 0.0
2022-07-04 at 11:48:10 Epoch: 2, Batch: 500, Loss: 0.0
2022-07-04 at 11:48:49 Epoch: 2, Batch: 600, Loss: 0.0
2022-07-04 at 11:49:27 Epoch: 2, Batch: 700, Loss: 0.0
2022-07-04 at 11:50:06 Epoch: 2, Batch: 800, Loss: 0.0
2022-07-04 at 11:50:44 Epoch: 2, Batch: 900, Loss: 0.0
2022-07-04 at 11:50:44 10 consecutive minimal loss, Epoch Early Skipped.

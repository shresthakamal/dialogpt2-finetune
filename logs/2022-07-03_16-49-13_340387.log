2022-07-03 at 16:49:13 {'batch': 4, 'epochs': 3, 'save': 'dialogpt-finetune', 'lr': 5e-05, 'clip': 2.0, 'seed': 1234, 'context': 3, 'max_len': 128, 'prepare': False, 'grad_accumulate': 4, 'tensorboard': 'runs/', 'early_stop': 10}
2022-07-03 at 16:49:13 Working on GPU: cuda
2022-07-03 at 16:49:13 Loading Tokenizer ...
2022-07-03 at 16:49:24 Loading Saved Dataset ...
2022-07-03 at 16:49:55 Loading the model ...
2022-07-03 at 16:50:08 
Training the model ...

2022-07-03 at 16:50:08 Epoch: 0, Batch: 0, Loss: 5.586581707000732
2022-07-03 at 16:50:48 Epoch: 0, Batch: 100, Loss: 1.4551914118143827e-09
2022-07-03 at 16:51:28 Epoch: 0, Batch: 200, Loss: 0.0
2022-07-03 at 16:52:08 Epoch: 0, Batch: 300, Loss: 0.0
2022-07-03 at 16:52:48 Epoch: 0, Batch: 400, Loss: 0.0
2022-07-03 at 16:53:28 Epoch: 0, Batch: 500, Loss: 0.0
2022-07-03 at 16:54:08 Epoch: 0, Batch: 600, Loss: 0.0
2022-07-03 at 16:54:48 Epoch: 0, Batch: 700, Loss: 0.0
2022-07-03 at 16:55:28 Epoch: 0, Batch: 800, Loss: 0.0
2022-07-03 at 16:56:08 Epoch: 0, Batch: 900, Loss: 0.0
2022-07-03 at 16:56:48 Epoch: 0, Batch: 1000, Loss: 0.0
2022-07-03 at 16:57:28 Epoch: 0, Batch: 1100, Loss: 0.0
2022-07-03 at 16:57:28 10 consecutive minimal loss, Epoch Early Skipped.
2022-07-03 at 16:57:30 Epoch: 1, Batch: 0, Loss: 0.0
2022-07-03 at 16:58:10 Epoch: 1, Batch: 100, Loss: 0.0

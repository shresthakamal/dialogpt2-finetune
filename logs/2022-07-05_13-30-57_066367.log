2022-07-05 at 13:30:57 Preparing Dataset ...
2022-07-05 at 13:30:57 Creating TensorDataset and DataLoader ...
2022-07-05 at 13:31:04 Training the model, num_training_steps: 57039
2022-07-05 at 13:31:07 --------------- Epoch: 0 ------------
2022-07-05 at 13:31:34 Epoch: 0, Batch: 100, Loss: 3.4312918376922608
2022-07-05 at 13:32:00 Epoch: 0, Batch: 200, Loss: 3.1474146211147307
2022-07-05 at 13:32:26 Epoch: 0, Batch: 300, Loss: 2.978240294456482
2022-07-05 at 13:32:52 Epoch: 0, Batch: 400, Loss: 2.986374181509018
2022-07-05 at 13:33:18 Epoch: 0, Batch: 500, Loss: 2.939514003992081
2022-07-05 at 13:33:44 Epoch: 0, Batch: 600, Loss: 2.860301659107208
2022-07-05 at 13:34:10 Epoch: 0, Batch: 700, Loss: 2.9009154903888703
2022-07-05 at 13:34:36 Epoch: 0, Batch: 800, Loss: 2.8375614273548124
2022-07-05 at 13:35:02 Epoch: 0, Batch: 900, Loss: 2.798328355550766
2022-07-05 at 13:35:28 Epoch: 0, Batch: 1000, Loss: 2.836083014011383
2022-07-05 at 13:35:55 Epoch: 0, Batch: 1100, Loss: 2.797973634004593
2022-07-05 at 13:36:21 Epoch: 0, Batch: 1200, Loss: 2.775449215173721
2022-07-05 at 13:36:47 Epoch: 0, Batch: 1300, Loss: 2.8506244790554045
2022-07-05 at 13:37:13 Epoch: 0, Batch: 1400, Loss: 2.8229601228237153
2022-07-05 at 13:37:39 Epoch: 0, Batch: 1500, Loss: 2.7262389624118804
2022-07-05 at 13:38:05 Epoch: 0, Batch: 1600, Loss: 2.8378285753726957
2022-07-05 at 13:38:31 Epoch: 0, Batch: 1700, Loss: 2.815268484354019
2022-07-05 at 13:38:57 Epoch: 0, Batch: 1800, Loss: 2.83034158706665
2022-07-05 at 13:39:23 Epoch: 0, Batch: 1900, Loss: 2.771685554981232
2022-07-05 at 13:39:49 Epoch: 0, Batch: 2000, Loss: 2.825487334728241
2022-07-05 at 13:40:15 Epoch: 0, Batch: 2100, Loss: 2.7283795285224914
2022-07-05 at 13:40:42 Epoch: 0, Batch: 2200, Loss: 2.808741465806961
2022-07-05 at 13:41:08 Epoch: 0, Batch: 2300, Loss: 2.8232484579086305
2022-07-05 at 13:41:34 Epoch: 0, Batch: 2400, Loss: 2.6976082360744478
2022-07-05 at 13:42:00 Epoch: 0, Batch: 2500, Loss: 2.69983146071434
2022-07-05 at 13:42:26 Epoch: 0, Batch: 2600, Loss: 2.703035867214203
2022-07-05 at 13:42:52 Epoch: 0, Batch: 2700, Loss: 2.734916158914566
2022-07-05 at 13:43:18 Epoch: 0, Batch: 2800, Loss: 2.766295189857483
2022-07-05 at 13:43:44 Epoch: 0, Batch: 2900, Loss: 2.707818466424942
2022-07-05 at 13:44:10 Epoch: 0, Batch: 3000, Loss: 2.730811599493027
2022-07-05 at 13:44:36 Epoch: 0, Batch: 3100, Loss: 2.7329793441295624
2022-07-05 at 13:45:02 Epoch: 0, Batch: 3200, Loss: 2.7295789706707
2022-07-05 at 13:45:29 Epoch: 0, Batch: 3300, Loss: 2.669574704170227
2022-07-05 at 13:45:55 Epoch: 0, Batch: 3400, Loss: 2.7095495688915254
2022-07-05 at 13:46:21 Epoch: 0, Batch: 3500, Loss: 2.6958962988853457
2022-07-05 at 13:46:47 Epoch: 0, Batch: 3600, Loss: 2.717002571821213
2022-07-05 at 13:47:13 Epoch: 0, Batch: 3700, Loss: 2.7658971190452575
2022-07-05 at 13:47:39 Epoch: 0, Batch: 3800, Loss: 2.6436163556575774
2022-07-05 at 13:48:05 Epoch: 0, Batch: 3900, Loss: 2.6344342434406283
2022-07-05 at 13:48:31 Epoch: 0, Batch: 4000, Loss: 2.650267332792282
2022-07-05 at 13:48:57 Epoch: 0, Batch: 4100, Loss: 2.652783567905426
2022-07-05 at 13:49:23 Epoch: 0, Batch: 4200, Loss: 2.6713765728473664
2022-07-05 at 13:49:49 Epoch: 0, Batch: 4300, Loss: 2.564575369358063
2022-07-05 at 13:50:16 Epoch: 0, Batch: 4400, Loss: 2.6850182580947877
2022-07-05 at 13:50:42 Epoch: 0, Batch: 4500, Loss: 2.5818393564224245
2022-07-05 at 13:51:08 Epoch: 0, Batch: 4600, Loss: 2.7047872281074525
2022-07-05 at 13:51:34 Epoch: 0, Batch: 4700, Loss: 2.743965371847153
2022-07-05 at 13:52:00 Epoch: 0, Batch: 4800, Loss: 2.625741482973099
2022-07-05 at 13:52:26 Epoch: 0, Batch: 4900, Loss: 2.655009788274765
2022-07-05 at 13:52:52 Epoch: 0, Batch: 5000, Loss: 2.6421603572368624
2022-07-05 at 13:53:18 Epoch: 0, Batch: 5100, Loss: 2.675630385875702
2022-07-05 at 13:53:45 Epoch: 0, Batch: 5200, Loss: 2.6322694003582
2022-07-05 at 13:54:11 Epoch: 0, Batch: 5300, Loss: 2.677380692958832
2022-07-05 at 13:54:37 Epoch: 0, Batch: 5400, Loss: 2.6851757764816284
2022-07-05 at 13:55:03 Epoch: 0, Batch: 5500, Loss: 2.592435871362686
2022-07-05 at 13:55:29 Epoch: 0, Batch: 5600, Loss: 2.597939828634262
2022-07-05 at 13:55:55 Epoch: 0, Batch: 5700, Loss: 2.625122170448303
2022-07-05 at 13:56:21 Epoch: 0, Batch: 5800, Loss: 2.666051949262619
2022-07-05 at 13:56:47 Epoch: 0, Batch: 5900, Loss: 2.6335724914073944
2022-07-05 at 13:57:13 Epoch: 0, Batch: 6000, Loss: 2.6378780019283297
2022-07-05 at 13:57:40 Epoch: 0, Batch: 6100, Loss: 2.56376881480217

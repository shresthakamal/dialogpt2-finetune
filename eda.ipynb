{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "from pathlib import Path\n",
            "import os"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "DATA_DIR = Path(\"data\", \"ijcnlp_dailydialog\", \"train\")\n",
            "data = pd.read_csv(Path(DATA_DIR, \"dialogues_train.txt\"),  delimiter = \"\\n\", names = [\"dialogues\"])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "def seputterances(row):\n",
            "    try:\n",
            "        row = row.split(\"__eou__\")\n",
            "        row = row[:-1]\n",
            "        return row\n",
            "    except:\n",
            "        return row\n",
            "\n",
            "data[\"dialogues\"] = data[\"dialogues\"].apply(seputterances)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/raid/cs18resch11003/anaconda3/envs/dialogpt/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                  "  from .autonotebook import tqdm as notebook_tqdm\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "<|endoftext|>\n"
               ]
            }
         ],
         "source": [
            "from transformers import AutoTokenizer\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "num_context = 3\n",
            "\n",
            "utterance = []\n",
            "history = []\n",
            "\n",
            "for i in data.index:\n",
            "\n",
            "    row = data[\"dialogues\"][i]\n",
            "\n",
            "    for idx  in range(len(row)):\n",
            "\n",
            "        if idx != 0:\n",
            "            \n",
            "            utterance.append(row[idx])\n",
            "\n",
            "            counter = 1\n",
            "            _history = \"\"\n",
            "            \n",
            "            for k in range(idx-1, -1, -1):\n",
            "                if counter <= num_context:\n",
            "                    _history = _history + row[k]\n",
            "                    counter +=1\n",
            "                else:\n",
            "                    break\n",
            "                _history = _history + tokenizer.eos_token\n",
            "            history.append(_history)\n",
            "        else:\n",
            "            continue"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
            "max_len = 32"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 91,
         "metadata": {},
         "outputs": [],
         "source": [
            "import torch\n",
            "\n",
            "input_ids = []\n",
            "\n",
            "attention_masks = []\n",
            "\n",
            "labels = []\n",
            "\n",
            "for i in range(len(utterance)):\n",
            "    \n",
            "    encoded_utterance = tokenizer.encode_plus(utterance[i].lower() + tokenizer.eos_token, max_length = max_len, padding= \"max_length\", truncation = True, return_tensors = \"pt\")\n",
            "    \n",
            "    encoded_history = tokenizer.encode_plus(history[i].lower(), max_length = max_len, truncation = True, padding= \"max_length\", return_tensors = \"pt\")\n",
            "\n",
            "    ids = torch.cat([encoded_utterance[\"input_ids\"][0], encoded_history[\"input_ids\"][0]], dim=0).reshape(1,max_len*2)\n",
            "    mask = torch.cat([encoded_utterance[\"attention_mask\"][0], encoded_history[\"attention_mask\"][0]], dim=0).reshape(1,max_len*2)\n",
            "    label = torch.cat([torch.full((max_len,), -100), torch.full((max_len,), 1)], dim = 0).reshape(1, max_len*2)\n",
            "\n",
            "    input_ids.append(ids)\n",
            "\n",
            "    attention_masks.append(mask)\n",
            "    labels.append(label)\n",
            "    if i == 10:\n",
            "        break"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 92,
         "metadata": {},
         "outputs": [],
         "source": [
            "input_ids = torch.cat(input_ids, dim = 0)\n",
            "attention_masks = torch.cat(attention_masks, dim=0)\n",
            "labels = torch.cat(labels, dim=0)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 93,
         "metadata": {},
         "outputs": [],
         "source": [
            "from torch.utils.data import TensorDataset\n",
            "\n",
            "dataset = TensorDataset(input_ids, attention_masks, labels)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 94,
         "metadata": {},
         "outputs": [],
         "source": [
            "from torch.utils.data import DataLoader, RandomSampler\n",
            "\n",
            "dataloader = DataLoader(\n",
            "            dataset,\n",
            "            sampler = RandomSampler(dataset),\n",
            "            batch_size = 8\n",
            "        )"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 97,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "tensor([[  345,   760,   326,   318, 29850,   475,   318,  1107,   407,   922,\n",
                  "           329,   674, 13547,   764,   220, 50256, 50257, 50257, 50257, 50257,\n",
                  "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
                  "         50257, 50257, 16706,   837,   474,   320,   837,   703,   546,  1016,\n",
                  "           329,   257,  1178, 16800,   706,  8073,  5633,   220, 50256, 50257,\n",
                  "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
                  "         50257, 50257, 50257, 50257],\n",
                  "        [ 5238,  1049,   284,   502,  5145,   611,   484,   389,  4684,   837,\n",
                  "           356,   714,  1265,   606,   284,   467, 15360,   351,   514,    13,\n",
                  "          5562,   318,  6275,  5517,   290,  1257,   837,  1165,   764,   220,\n",
                  "         50256, 50257,   326,   338,   257,   922,  2126,   764,  1312,  3285,\n",
                  "           285,   560,   290,   264,   453,  1690,   467,   612,   284,   711,\n",
                  "         29400,    79,   506,    13, 28998,   356,   460,   787,   257,  1440,\n",
                  "         11246,   351,   606,   764],\n",
                  "        [  922,    13,  1616,   705,   264,   467,   783,   764,   220, 50256,\n",
                  "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
                  "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
                  "         50257, 50257,  5238,  1049,   284,   502,  5145,   611,   484,   389,\n",
                  "          4684,   837,   356,   714,  1265,   606,   284,   467, 15360,   351,\n",
                  "           514,    13,  5562,   318,  6275,  5517,   290,  1257,   837,  1165,\n",
                  "           764,   220, 50256,   326],\n",
                  "        [ 1312,  4724,   345,   389,   826,    13,  4360,   644,  2236,   356,\n",
                  "           466,  5633,  1312,   836,   470,  1254,   588,  5586,   379,  1363,\n",
                  "           764,   220, 50256, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
                  "         50257, 50257,   466,   345,  1107,   892,   523,  5633,  1312,   836,\n",
                  "           470,   764,   340,   481,   655,   787,   514,  3735,   290,   719,\n",
                  "         14397,   764,  3505,   938,   640,  5633,   220, 50256,   644,   466,\n",
                  "           345,  1612,  5633,   340],\n",
                  "        [ 1107,  5633,  1312,   892,   326,   338,  5340,  5145,   220, 50256,\n",
                  "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
                  "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
                  "         50257, 50257,   286,  1781,  1312,   460,   764,   340,   338,   257,\n",
                  "          3704,   286, 12187,  5145,  1975,   340,   393,   407,   837,  1312,\n",
                  "           460,   466,  1542,  4574,    12,  4739,   257,  5664,   764,   220,\n",
                  "         50256,  5171,   345,   466],\n",
                  "        [  286,  1781,  1312,   460,   764,   340,   338,   257,  3704,   286,\n",
                  "         12187,  5145,  1975,   340,   393,   407,   837,  1312,   460,   466,\n",
                  "          1542,  4574,    12,  4739,   257,  5664,   764,   220, 50256, 50257,\n",
                  "         50257, 50257,  5171,   345,   466,  4574,    12,  4739,  5633,   220,\n",
                  "         50256, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
                  "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
                  "         50257, 50257, 50257, 50257],\n",
                  "        [  644,   466,   345,  1612,  5633,   340,   481,  1037,   514,   284,\n",
                  "          8960,   764,   220, 50256, 50257, 50257, 50257, 50257, 50257, 50257,\n",
                  "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
                  "         50257, 50257,   345,   760,   326,   318, 29850,   475,   318,  1107,\n",
                  "           407,   922,   329,   674, 13547,   764,   220, 50256, 16706,   837,\n",
                  "           474,   320,   837,   703,   546,  1016,   329,   257,  1178, 16800,\n",
                  "           706,  8073,  5633,   220],\n",
                  "        [  477,   826,   764,   220, 50256, 50257, 50257, 50257, 50257, 50257,\n",
                  "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
                  "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
                  "         50257, 50257,   922,    13,  1616,   705,   264,   467,   783,   764,\n",
                  "           220, 50256,  5238,  1049,   284,   502,  5145,   611,   484,   389,\n",
                  "          4684,   837,   356,   714,  1265,   606,   284,   467, 15360,   351,\n",
                  "           514,    13,  5562,   318]]) tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                  "         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                  "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
                  "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                  "         1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                  "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
                  "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                  "         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                  "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
                  "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
                  "         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                  "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
                  "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                  "         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                  "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
                  "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                  "         1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
                  "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
                  "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                  "         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                  "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
                  "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                  "         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                  "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
                  "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
                  "         -100, -100, -100, -100, -100, -100, -100, -100,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1],\n",
                  "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
                  "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
                  "         -100, -100, -100, -100, -100, -100, -100, -100,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1],\n",
                  "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
                  "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
                  "         -100, -100, -100, -100, -100, -100, -100, -100,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1],\n",
                  "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
                  "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
                  "         -100, -100, -100, -100, -100, -100, -100, -100,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1],\n",
                  "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
                  "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
                  "         -100, -100, -100, -100, -100, -100, -100, -100,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1],\n",
                  "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
                  "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
                  "         -100, -100, -100, -100, -100, -100, -100, -100,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1],\n",
                  "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
                  "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
                  "         -100, -100, -100, -100, -100, -100, -100, -100,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1],\n",
                  "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
                  "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
                  "         -100, -100, -100, -100, -100, -100, -100, -100,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1]])\n",
                  "tensor([[ 1312,  1950,   257,  2513,   625,   284,   262, 11550,   810,   356,\n",
                  "           460,   711, 33041,   506,   290,  1826,   617,   286,   674,  2460,\n",
                  "           764,   220, 50256, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
                  "         50257, 50257,  1312,  4724,   345,   389,   826,    13,  4360,   644,\n",
                  "          2236,   356,   466,  5633,  1312,   836,   470,  1254,   588,  5586,\n",
                  "           379,  1363,   764,   220, 50256,   466,   345,  1107,   892,   523,\n",
                  "          5633,  1312,   836,   470],\n",
                  "        [  466,   345,  1107,   892,   523,  5633,  1312,   836,   470,   764,\n",
                  "           340,   481,   655,   787,   514,  3735,   290,   719, 14397,   764,\n",
                  "          3505,   938,   640,  5633,   220, 50256, 50257, 50257, 50257, 50257,\n",
                  "         50257, 50257,   644,   466,   345,  1612,  5633,   340,   481,  1037,\n",
                  "           514,   284,  8960,   764,   220, 50256,   345,   760,   326,   318,\n",
                  "         29850,   475,   318,  1107,   407,   922,   329,   674, 13547,   764,\n",
                  "           220, 50256, 16706,   837],\n",
                  "        [  326,   338,   257,   922,  2126,   764,  1312,  3285,   285,   560,\n",
                  "           290,   264,   453,  1690,   467,   612,   284,   711, 29400,    79,\n",
                  "           506,    13, 28998,   356,   460,   787,   257,  1440, 11246,   351,\n",
                  "           606,   764,  1312,  1950,   257,  2513,   625,   284,   262, 11550,\n",
                  "           810,   356,   460,   711, 33041,   506,   290,  1826,   617,   286,\n",
                  "           674,  2460,   764,   220, 50256,  1312,  4724,   345,   389,   826,\n",
                  "            13,  4360,   644,  2236]]) tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
                  "         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                  "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
                  "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                  "         1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                  "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
                  "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                  "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                  "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
                  "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
                  "         -100, -100, -100, -100, -100, -100, -100, -100,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1],\n",
                  "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
                  "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
                  "         -100, -100, -100, -100, -100, -100, -100, -100,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1],\n",
                  "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
                  "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
                  "         -100, -100, -100, -100, -100, -100, -100, -100,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
                  "            1,    1,    1,    1]])\n"
               ]
            }
         ],
         "source": [
            "for i, batch in enumerate(dataloader):\n",
            "    print(batch[0], batch[1], batch[2])\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": 117,
         "metadata": {},
         "outputs": [],
         "source": [
            "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "import torch\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 118,
         "metadata": {},
         "outputs": [],
         "source": [
            "dataset = {\"S1\":[], \"S2\": []}\n",
            "\n",
            "\n",
            "for i in range(len(data)):\n",
            "    for j in range(len(data[\"dialogues\"][i])):\n",
            "        dataset[\"S1\"].append(data[\"dialogues\"][i][j])\n",
            "        try:\n",
            "            dataset[\"S2\"].append(data[\"dialogues\"][i][j+1])\n",
            "        except:\n",
            "            dataset[\"S2\"].append(tokenizer.eos_token)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 119,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "{'input_ids': tensor([[25515,   837,  5395,  ..., 50256, 50256, 50256],\n",
                     "        [  921,   760,   326,  ..., 50256, 50256, 50256],\n",
                     "        [ 1867,   466,   345,  ..., 50256, 50256, 50256],\n",
                     "        ...,\n",
                     "        [  314,   761,  2546,  ..., 50256, 50256, 50256],\n",
                     "        [23722,   314,   423,  ..., 50256, 50256, 50256],\n",
                     "        [16805,   764,   314,  ..., 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
                     "        [1, 1, 1,  ..., 0, 0, 0],\n",
                     "        [1, 1, 1,  ..., 0, 0, 0],\n",
                     "        ...,\n",
                     "        [1, 1, 1,  ..., 0, 0, 0],\n",
                     "        [1, 1, 1,  ..., 0, 0, 0],\n",
                     "        [1, 1, 1,  ..., 0, 0, 0]])}"
                  ]
               },
               "execution_count": 119,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "tokenizer.pad_token = tokenizer.eos_token\n",
            "\n",
            "tokenized_dataset = tokenizer(\n",
            "    dataset[\"S1\"],\n",
            "    dataset[\"S2\"],\n",
            "    padding=True,\n",
            "    truncation=True,\n",
            "    return_tensors = \"pt\"\n",
            ")\n",
            "\n",
            "tokenized_dataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Using custom data configuration default\n",
                  "Reusing dataset daily_dialog (/home/cs18resch11003/.cache/huggingface/datasets/daily_dialog/default/1.0.0/1d0a58c7f2a4dab5ed9d01dbde8e55e0058e589ab81fce5c2df929ea810eabcd)\n",
                  "100%|██████████| 3/3 [00:00<00:00, 469.06it/s]\n"
               ]
            }
         ],
         "source": [
            "import pickle \n",
            "\n",
            "dataset = load_dataset('daily_dialog')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "Dataset({\n",
                     "    features: ['dialog', 'act', 'emotion'],\n",
                     "    num_rows: 11118\n",
                     "})"
                  ]
               },
               "execution_count": 7,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "train_dataset = dataset[\"train\"]\n",
            "train_dataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [],
         "source": [
            "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "import torch\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [],
         "source": [
            "tokenizer.pad_token = tokenizer.eos_token\n",
            "\n",
            "tokenized_dataset = tokenizer(\n",
            "    train_dataset[\"dialog\"][0],\n",
            "    train_dataset[\"dialog\"][0],\n",
            "    padding=True,\n",
            "    truncation=True,\n",
            "    return_tensors = \"pt\"\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 39,
         "metadata": {},
         "outputs": [],
         "source": [
            "tokenizer.pad_token = tokenizer.eos_token\n",
            "\n",
            "def tokenize_function(example):\n",
            "    print(example[\"dialog\"][0])\n",
            "    exit"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 40,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  " 50%|█████     | 6/12 [00:00<00:00, 28.05ba/s]"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "['Say , Jim , how about going for a few beers after dinner ? ', ' You know that is tempting but is really not good for our fitness . ', ' What do you mean ? It will help us to relax . ', \" Do you really think so ? I don't . It will just make us fat and act silly . Remember last time ? \", \" I guess you are right.But what shall we do ? I don't feel like sitting at home . \", ' I suggest a walk over to the gym where we can play singsong and meet some of our friends . ', \" That's a good idea . I hear Mary and Sally often go there to play pingpong.Perhaps we can make a foursome with them . \", ' Sounds great to me ! If they are willing , we could ask them to go dancing with us.That is excellent exercise and fun , too . ', \" Good.Let ' s go now . \", ' All right . ']\n",
                  "['Great party , isn ’ t it ? ', ' Yeah , really . ', ' By the way , my name is Liu Wei . ', ' Hi , I ’ m Susan Marshall , You can call me Susan.Nice to meet you . ', ' What do you do , Susan ? ', ' Well , I ’ m a college student.How about you ? ', ' I work for the Bank of China , in the International Section . ']\n",
                  "['I noticed your absence in class this morning . ', \" I'm sorry , I overslept . \"]\n",
                  "['Bill , I have received the admission letter from Cambridge University ! ', ' Congratulations ! ', ' How about you ? Have you got any news from university ? ', ' No . But I am going to apply for some vocational schools . I like fashion design and want to study it . ', ' I am glad you can study something you really like ! ']\n",
                  "['Mrs . Smith , you are always worried about your children . ', ' They are my own flesh and blood . It ’ s hard for me to see them suffering . ', ' That ’ s human nature . ']\n",
                  "['I like this shirt.How much is it ? ', ' $ 135 . ', \" Wow , that's sheer robbery.Shall I make an offer ? \", \" I'm sorry , sir.The price is fixed . \"]\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "100%|██████████| 12/12 [00:00<00:00, 30.39ba/s]"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "['Do you have this design with only one breast pocket ? ', ' Let me see.Oh , we have the design but not the same color as this one . ', \" It's a pity . I'm afraid that's too loud for me.Thank you . \", \" You're welcome . \"]\n",
                  "['I have been getting headaches almost every day lately . ', ' Have you just started getting a lot of headaches ? ', ' I never had very many headaches before , but the last few weeks I have been getting a lot of them . ', ' Have you had any unusually stressful situations in your life lately ? ', ' My mother just passed away last Tuesday . ', ' I ’ m sorry . How about sleep ? Are you getting enough rest ? ', ' I have been working really hard , and sleep has not been a priority . ', ' Have you bumped your head or fallen lately ? ', ' No , I haven ’ t hit my head . ', ' I am going to send you to a neurologist for a few tests . ']\n",
                  "['May I help you ? Miss ! ', \" Yes . I'd like to look at lipstick and eye shadow . \", ' What color set do you prefer ? ', ' Well , brown . ', \" We have a beautiful selection of eye shadows this fall . Look at the colors . Aren't they beautiful ? \", \" But they're purple . I prefer a brown set . \", \" If you insist , I can show you the brown sets . I'll have to warn you that they're very ordinary , though . \", \" Well , I'm not so sure . Most of my make-up is brown . \", \" Why don't you wear purple eye shadow for a change ? We also have lipstick to go with it . \", ' Can I try it ? ', ' Sure . Are you wearing any make-up ? ', ' No . ', ' Have a seat , please . Now , here is the mirror . How do you like it ? ', ' Not bad . Actually , it makes me look younger . I like it . ', ' Try the lipstick as well . See , how fresh and charming you look . ', \" You're right . I'll take them all . \"]\n",
                  "[\"Hi , I'm checking in . The last name is Rama . \", ' Yes , here is your reservation . You have a standard room reserved for two nights . Is that right ? ', ' Actually , no . It should be a suite . I had booked a non-smoking king . ', \" Oh , my mistake . The reservation is for a suite and it is a non-smoking room with a king bed . I'm sorry for the error . \", \" That's okay . I'm here a little early . Is it possible to check in right now ? \", \" Sure , that's no problem . May I have your credit card ? We need a credit card on file for your room charges and incidentals . \", ' Here it is . ', ' Okay , now if you could please verify the room rate here , initial next to the X , and sign right here . How many keys will you need ? ', ' Oh , just one . ', \" Okay , you're all set . You're in room 1201 . Take the elevators to the 12th floor and it will be on your left . Do you need any help with your bags ? \", \" No , I'm fine . Thanks . \", ' Enjoy your stay . ']\n",
                  "[\"Okay , here are the graphs and figures for this month's sales . Let's review them all together . \", \" This first one , I have a question ... This graph is marking the sales performance for our line of hair products , right ? Can this line be right ? It looks like our sales plummeted . I can't believe we did that poorly ... If I remember correctly , sales went down slightly , but not as dramatically as the graph shows . \", \" I think you are looking at the wrong line . The rapid drop in sales wasn't our hair products . You are correct , the hair product sales decreased slightly , but not dramatically . The one that didn't do so hot this month was the cleaning products . I think there was a problem in the marketing plan . Some people were offended by our advertisements for the cleaning products , but it was already too late to mitigate the damage , so our mistake shows up in the sales . \", ' Well , the good news is the new industrial cleaning products really took off . Look how the sales have shot up over the last two weeks . ', ' That is our one major success . If you look at the other graphs , you can see that most of the other product lines remained steady with little increase . ', \" At lease they stayed the same . That's better than dropping . \"]\n",
                  "['Any new ideas on the Mr . Fro case ? How can we give this motor oil a new brand identity ? ', ' Let me get my morning coffee to get my brain working again . ', \" I could use one , too . I'm still burnt out from that last cigarette ad campaign . \", \" As long as the foam on my macchiato's not burnt , too , I'll be alright for a while . You got your espresso , Jess ? \"]\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "Dataset({\n",
                     "    features: ['dialog', 'act', 'emotion'],\n",
                     "    num_rows: 11118\n",
                     "})"
                  ]
               },
               "execution_count": 40,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "tokenized_datasets = train_dataset.map(tokenize_function, batched=True)\n",
            "tokenized_datasets"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": 57,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": 123,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "True"
                  ]
               },
               "execution_count": 123,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "import os.path\n",
            "os.path.isfile(Path(\"data\", \"inturn_conversations.pkl\")) "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 105,
         "metadata": {},
         "outputs": [],
         "source": [
            "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 108,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "77235 cuda\n"
               ]
            }
         ],
         "source": [
            "import torch\n",
            "from transformers import AdamW\n",
            "from transformers import get_scheduler\n",
            "\n",
            "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
            "\n",
            "num_epochs = 3\n",
            "\n",
            "num_batch = 2\n",
            "\n",
            "num_training_steps = int(num_epochs * len(tokenized_dataset[\"input_ids\"])/4)\n",
            "lr_scheduler = get_scheduler(\n",
            "    \"linear\",\n",
            "    optimizer=optimizer,\n",
            "    num_warmup_steps=0,\n",
            "    num_training_steps=num_training_steps,\n",
            ")\n",
            "\n",
            "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
            "model.to(device)\n",
            "\n",
            "print(num_training_steps, device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 109,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "  0%|          | 0/77235 [00:10<?, ?it/s]\n"
               ]
            },
            {
               "ename": "RuntimeError",
               "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.90 GiB total capacity; 4.54 GiB already allocated; 23.75 MiB free; 4.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
                  "\u001b[0;32m/tmp/ipykernel_3200337/4082497537.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/raid/cs18resch11003/anaconda3/envs/dialogpt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/raid/cs18resch11003/anaconda3/envs/dialogpt/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m         )\n\u001b[1;32m   1063\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/raid/cs18resch11003/anaconda3/envs/dialogpt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/raid/cs18resch11003/anaconda3/envs/dialogpt/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m                 )\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/raid/cs18resch11003/anaconda3/envs/dialogpt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/raid/cs18resch11003/anaconda3/envs/dialogpt/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         )\n\u001b[1;32m    399\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# output_attn: a, present, (attentions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/raid/cs18resch11003/anaconda3/envs/dialogpt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/raid/cs18resch11003/anaconda3/envs/dialogpt/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/raid/cs18resch11003/anaconda3/envs/dialogpt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/raid/cs18resch11003/anaconda3/envs/dialogpt/lib/python3.7/site-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0msize_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.90 GiB total capacity; 4.54 GiB already allocated; 23.75 MiB free; 4.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
               ]
            }
         ],
         "source": [
            "from tqdm.auto import tqdm\n",
            "\n",
            "progress_bar = tqdm(range(num_training_steps))\n",
            "\n",
            "model.train()\n",
            "\n",
            "for epoch in range(num_epochs):\n",
            "    for i in range(len(tokenized_dataset[\"input_ids\"])):\n",
            "        inputs= {}\n",
            "\n",
            "        ids = tokenized_dataset[\"input_ids\"][:num_batch]\n",
            "        mask = tokenized_dataset[\"attention_mask\"][:num_batch]\n",
            "\n",
            "        inputs[\"input_ids\"] = ids.to(device)\n",
            "        inputs[\"attention_mask\"] = mask.to(device)\n",
            "\n",
            "        outputs = model(**inputs, labels = inputs[\"input_ids\"])\n",
            "        print(outputs)\n",
            "\n",
            "        loss = outputs.loss\n",
            "        \n",
            "        loss.backward()\n",
            "        optimizer.step()\n",
            "        lr_scheduler.step()\n",
            "        optimizer.zero_grad()\n",
            "        \n",
            "        progress_bar.update(1)\n",
            "        \n",
            "        break\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": 58,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3.7.13 ('dialogpt')",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.7.13"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "47a16a6705441b6dda92a294eb31436c27952dbca87d6ec649ac2e31022a0dfe"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}

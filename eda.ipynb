{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 113,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "from pathlib import Path\n",
            "import os\n",
            "\n",
            "\n",
            "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
            "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 114,
         "metadata": {},
         "outputs": [],
         "source": [
            "DATA_DIR = Path(\"data\", \"ijcnlp_dailydialog\", \"train\")\n",
            "data = pd.read_csv(Path(DATA_DIR, \"dialogues_train.txt\"),  delimiter = \"\\n\", names = [\"dialogues\"])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 116,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>dialogues</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>[Say , Jim , how about going for a few beers a...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>1</th>\n",
                     "      <td>[Can you do push-ups ? ,  Of course I can . It...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>2</th>\n",
                     "      <td>[Can you study with the radio on ? ,  No , I l...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>3</th>\n",
                     "      <td>[Are you all right ? ,  I will be all right so...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>4</th>\n",
                     "      <td>[Hey John , nice skates . Are they new ? ,  Ye...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>...</th>\n",
                     "      <td>...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>11113</th>\n",
                     "      <td>[Hello , I bought a pen in your shop just befo...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>11114</th>\n",
                     "      <td>[Do you have any seats available ? ,  Yes . Th...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>11115</th>\n",
                     "      <td>[Uncle Ben , how did the Forbidden City get th...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>11116</th>\n",
                     "      <td>[May I help you , sir ? ,  I want a pair of lo...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>11117</th>\n",
                     "      <td>[Could I have the check , please ? ,  Okay . I...</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "<p>11118 rows × 1 columns</p>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "                                               dialogues\n",
                     "0      [Say , Jim , how about going for a few beers a...\n",
                     "1      [Can you do push-ups ? ,  Of course I can . It...\n",
                     "2      [Can you study with the radio on ? ,  No , I l...\n",
                     "3      [Are you all right ? ,  I will be all right so...\n",
                     "4      [Hey John , nice skates . Are they new ? ,  Ye...\n",
                     "...                                                  ...\n",
                     "11113  [Hello , I bought a pen in your shop just befo...\n",
                     "11114  [Do you have any seats available ? ,  Yes . Th...\n",
                     "11115  [Uncle Ben , how did the Forbidden City get th...\n",
                     "11116  [May I help you , sir ? ,  I want a pair of lo...\n",
                     "11117  [Could I have the check , please ? ,  Okay . I...\n",
                     "\n",
                     "[11118 rows x 1 columns]"
                  ]
               },
               "execution_count": 116,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "def seputterances(row):\n",
            "    try:\n",
            "        row = row.split(\"__eou__\")\n",
            "        row = row[:-1]\n",
            "        return row\n",
            "    except:\n",
            "        return row\n",
            "\n",
            "data[\"dialogues\"] = data[\"dialogues\"].apply(seputterances)\n",
            "\n",
            "data"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 117,
         "metadata": {},
         "outputs": [],
         "source": [
            "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "import torch\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 118,
         "metadata": {},
         "outputs": [],
         "source": [
            "dataset = {\"S1\":[], \"S2\": []}\n",
            "\n",
            "\n",
            "for i in range(len(data)):\n",
            "    for j in range(len(data[\"dialogues\"][i])):\n",
            "        dataset[\"S1\"].append(data[\"dialogues\"][i][j])\n",
            "        try:\n",
            "            dataset[\"S2\"].append(data[\"dialogues\"][i][j+1])\n",
            "        except:\n",
            "            dataset[\"S2\"].append(tokenizer.eos_token)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 119,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "{'input_ids': tensor([[25515,   837,  5395,  ..., 50256, 50256, 50256],\n",
                     "        [  921,   760,   326,  ..., 50256, 50256, 50256],\n",
                     "        [ 1867,   466,   345,  ..., 50256, 50256, 50256],\n",
                     "        ...,\n",
                     "        [  314,   761,  2546,  ..., 50256, 50256, 50256],\n",
                     "        [23722,   314,   423,  ..., 50256, 50256, 50256],\n",
                     "        [16805,   764,   314,  ..., 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
                     "        [1, 1, 1,  ..., 0, 0, 0],\n",
                     "        [1, 1, 1,  ..., 0, 0, 0],\n",
                     "        ...,\n",
                     "        [1, 1, 1,  ..., 0, 0, 0],\n",
                     "        [1, 1, 1,  ..., 0, 0, 0],\n",
                     "        [1, 1, 1,  ..., 0, 0, 0]])}"
                  ]
               },
               "execution_count": 119,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "tokenizer.pad_token = tokenizer.eos_token\n",
            "\n",
            "tokenized_dataset = tokenizer(\n",
            "    dataset[\"S1\"],\n",
            "    dataset[\"S2\"],\n",
            "    padding=True,\n",
            "    truncation=True,\n",
            "    return_tensors = \"pt\"\n",
            ")\n",
            "\n",
            "tokenized_dataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Using custom data configuration default\n",
                  "Reusing dataset daily_dialog (/home/cs18resch11003/.cache/huggingface/datasets/daily_dialog/default/1.0.0/1d0a58c7f2a4dab5ed9d01dbde8e55e0058e589ab81fce5c2df929ea810eabcd)\n",
                  "100%|██████████| 3/3 [00:00<00:00, 469.06it/s]\n"
               ]
            }
         ],
         "source": [
            "import pickle \n",
            "\n",
            "dataset = load_dataset('daily_dialog')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "Dataset({\n",
                     "    features: ['dialog', 'act', 'emotion'],\n",
                     "    num_rows: 11118\n",
                     "})"
                  ]
               },
               "execution_count": 7,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "train_dataset = dataset[\"train\"]\n",
            "train_dataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [],
         "source": [
            "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "import torch\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [],
         "source": [
            "tokenizer.pad_token = tokenizer.eos_token\n",
            "\n",
            "tokenized_dataset = tokenizer(\n",
            "    train_dataset[\"dialog\"][0],\n",
            "    train_dataset[\"dialog\"][0],\n",
            "    padding=True,\n",
            "    truncation=True,\n",
            "    return_tensors = \"pt\"\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 39,
         "metadata": {},
         "outputs": [],
         "source": [
            "tokenizer.pad_token = tokenizer.eos_token\n",
            "\n",
            "def tokenize_function(example):\n",
            "    print(example[\"dialog\"][0])\n",
            "    exit"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 40,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  " 50%|█████     | 6/12 [00:00<00:00, 28.05ba/s]"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "['Say , Jim , how about going for a few beers after dinner ? ', ' You know that is tempting but is really not good for our fitness . ', ' What do you mean ? It will help us to relax . ', \" Do you really think so ? I don't . It will just make us fat and act silly . Remember last time ? \", \" I guess you are right.But what shall we do ? I don't feel like sitting at home . \", ' I suggest a walk over to the gym where we can play singsong and meet some of our friends . ', \" That's a good idea . I hear Mary and Sally often go there to play pingpong.Perhaps we can make a foursome with them . \", ' Sounds great to me ! If they are willing , we could ask them to go dancing with us.That is excellent exercise and fun , too . ', \" Good.Let ' s go now . \", ' All right . ']\n",
                  "['Great party , isn ’ t it ? ', ' Yeah , really . ', ' By the way , my name is Liu Wei . ', ' Hi , I ’ m Susan Marshall , You can call me Susan.Nice to meet you . ', ' What do you do , Susan ? ', ' Well , I ’ m a college student.How about you ? ', ' I work for the Bank of China , in the International Section . ']\n",
                  "['I noticed your absence in class this morning . ', \" I'm sorry , I overslept . \"]\n",
                  "['Bill , I have received the admission letter from Cambridge University ! ', ' Congratulations ! ', ' How about you ? Have you got any news from university ? ', ' No . But I am going to apply for some vocational schools . I like fashion design and want to study it . ', ' I am glad you can study something you really like ! ']\n",
                  "['Mrs . Smith , you are always worried about your children . ', ' They are my own flesh and blood . It ’ s hard for me to see them suffering . ', ' That ’ s human nature . ']\n",
                  "['I like this shirt.How much is it ? ', ' $ 135 . ', \" Wow , that's sheer robbery.Shall I make an offer ? \", \" I'm sorry , sir.The price is fixed . \"]\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "100%|██████████| 12/12 [00:00<00:00, 30.39ba/s]"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "['Do you have this design with only one breast pocket ? ', ' Let me see.Oh , we have the design but not the same color as this one . ', \" It's a pity . I'm afraid that's too loud for me.Thank you . \", \" You're welcome . \"]\n",
                  "['I have been getting headaches almost every day lately . ', ' Have you just started getting a lot of headaches ? ', ' I never had very many headaches before , but the last few weeks I have been getting a lot of them . ', ' Have you had any unusually stressful situations in your life lately ? ', ' My mother just passed away last Tuesday . ', ' I ’ m sorry . How about sleep ? Are you getting enough rest ? ', ' I have been working really hard , and sleep has not been a priority . ', ' Have you bumped your head or fallen lately ? ', ' No , I haven ’ t hit my head . ', ' I am going to send you to a neurologist for a few tests . ']\n",
                  "['May I help you ? Miss ! ', \" Yes . I'd like to look at lipstick and eye shadow . \", ' What color set do you prefer ? ', ' Well , brown . ', \" We have a beautiful selection of eye shadows this fall . Look at the colors . Aren't they beautiful ? \", \" But they're purple . I prefer a brown set . \", \" If you insist , I can show you the brown sets . I'll have to warn you that they're very ordinary , though . \", \" Well , I'm not so sure . Most of my make-up is brown . \", \" Why don't you wear purple eye shadow for a change ? We also have lipstick to go with it . \", ' Can I try it ? ', ' Sure . Are you wearing any make-up ? ', ' No . ', ' Have a seat , please . Now , here is the mirror . How do you like it ? ', ' Not bad . Actually , it makes me look younger . I like it . ', ' Try the lipstick as well . See , how fresh and charming you look . ', \" You're right . I'll take them all . \"]\n",
                  "[\"Hi , I'm checking in . The last name is Rama . \", ' Yes , here is your reservation . You have a standard room reserved for two nights . Is that right ? ', ' Actually , no . It should be a suite . I had booked a non-smoking king . ', \" Oh , my mistake . The reservation is for a suite and it is a non-smoking room with a king bed . I'm sorry for the error . \", \" That's okay . I'm here a little early . Is it possible to check in right now ? \", \" Sure , that's no problem . May I have your credit card ? We need a credit card on file for your room charges and incidentals . \", ' Here it is . ', ' Okay , now if you could please verify the room rate here , initial next to the X , and sign right here . How many keys will you need ? ', ' Oh , just one . ', \" Okay , you're all set . You're in room 1201 . Take the elevators to the 12th floor and it will be on your left . Do you need any help with your bags ? \", \" No , I'm fine . Thanks . \", ' Enjoy your stay . ']\n",
                  "[\"Okay , here are the graphs and figures for this month's sales . Let's review them all together . \", \" This first one , I have a question ... This graph is marking the sales performance for our line of hair products , right ? Can this line be right ? It looks like our sales plummeted . I can't believe we did that poorly ... If I remember correctly , sales went down slightly , but not as dramatically as the graph shows . \", \" I think you are looking at the wrong line . The rapid drop in sales wasn't our hair products . You are correct , the hair product sales decreased slightly , but not dramatically . The one that didn't do so hot this month was the cleaning products . I think there was a problem in the marketing plan . Some people were offended by our advertisements for the cleaning products , but it was already too late to mitigate the damage , so our mistake shows up in the sales . \", ' Well , the good news is the new industrial cleaning products really took off . Look how the sales have shot up over the last two weeks . ', ' That is our one major success . If you look at the other graphs , you can see that most of the other product lines remained steady with little increase . ', \" At lease they stayed the same . That's better than dropping . \"]\n",
                  "['Any new ideas on the Mr . Fro case ? How can we give this motor oil a new brand identity ? ', ' Let me get my morning coffee to get my brain working again . ', \" I could use one , too . I'm still burnt out from that last cigarette ad campaign . \", \" As long as the foam on my macchiato's not burnt , too , I'll be alright for a while . You got your espresso , Jess ? \"]\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "Dataset({\n",
                     "    features: ['dialog', 'act', 'emotion'],\n",
                     "    num_rows: 11118\n",
                     "})"
                  ]
               },
               "execution_count": 40,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "tokenized_datasets = train_dataset.map(tokenize_function, batched=True)\n",
            "tokenized_datasets"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": 57,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": 123,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "True"
                  ]
               },
               "execution_count": 123,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "import os.path\n",
            "os.path.isfile(Path(\"data\", \"inturn_conversations.pkl\")) "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 105,
         "metadata": {},
         "outputs": [],
         "source": [
            "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 108,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "77235 cuda\n"
               ]
            }
         ],
         "source": [
            "import torch\n",
            "from transformers import AdamW\n",
            "from transformers import get_scheduler\n",
            "\n",
            "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
            "\n",
            "num_epochs = 3\n",
            "\n",
            "num_batch = 2\n",
            "\n",
            "num_training_steps = int(num_epochs * len(tokenized_dataset[\"input_ids\"])/4)\n",
            "lr_scheduler = get_scheduler(\n",
            "    \"linear\",\n",
            "    optimizer=optimizer,\n",
            "    num_warmup_steps=0,\n",
            "    num_training_steps=num_training_steps,\n",
            ")\n",
            "\n",
            "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
            "model.to(device)\n",
            "\n",
            "print(num_training_steps, device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 109,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "  0%|          | 0/77235 [00:10<?, ?it/s]\n"
               ]
            },
            {
               "ename": "RuntimeError",
               "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.90 GiB total capacity; 4.54 GiB already allocated; 23.75 MiB free; 4.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
                  "\u001b[0;32m/tmp/ipykernel_3200337/4082497537.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/raid/cs18resch11003/anaconda3/envs/dialogpt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/raid/cs18resch11003/anaconda3/envs/dialogpt/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m         )\n\u001b[1;32m   1063\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/raid/cs18resch11003/anaconda3/envs/dialogpt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/raid/cs18resch11003/anaconda3/envs/dialogpt/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m                 )\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/raid/cs18resch11003/anaconda3/envs/dialogpt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/raid/cs18resch11003/anaconda3/envs/dialogpt/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         )\n\u001b[1;32m    399\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# output_attn: a, present, (attentions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/raid/cs18resch11003/anaconda3/envs/dialogpt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/raid/cs18resch11003/anaconda3/envs/dialogpt/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/raid/cs18resch11003/anaconda3/envs/dialogpt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/raid/cs18resch11003/anaconda3/envs/dialogpt/lib/python3.7/site-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0msize_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.90 GiB total capacity; 4.54 GiB already allocated; 23.75 MiB free; 4.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
               ]
            }
         ],
         "source": [
            "from tqdm.auto import tqdm\n",
            "\n",
            "progress_bar = tqdm(range(num_training_steps))\n",
            "\n",
            "model.train()\n",
            "\n",
            "for epoch in range(num_epochs):\n",
            "    for i in range(len(tokenized_dataset[\"input_ids\"])):\n",
            "        inputs= {}\n",
            "\n",
            "        ids = tokenized_dataset[\"input_ids\"][:num_batch]\n",
            "        mask = tokenized_dataset[\"attention_mask\"][:num_batch]\n",
            "\n",
            "        inputs[\"input_ids\"] = ids.to(device)\n",
            "        inputs[\"attention_mask\"] = mask.to(device)\n",
            "\n",
            "        outputs = model(**inputs, labels = inputs[\"input_ids\"])\n",
            "        print(outputs)\n",
            "\n",
            "        loss = outputs.loss\n",
            "        \n",
            "        loss.backward()\n",
            "        optimizer.step()\n",
            "        lr_scheduler.step()\n",
            "        optimizer.zero_grad()\n",
            "        \n",
            "        progress_bar.update(1)\n",
            "        \n",
            "        break\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": 58,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3.7.13 ('dialogpt')",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.7.13"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "47a16a6705441b6dda92a294eb31436c27952dbca87d6ec649ac2e31022a0dfe"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "from pathlib import Path\n",
            "import os\n",
            "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
            "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
            "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [],
         "source": [
            "DATA_DIR = Path(\"data\", \"ijcnlp_dailydialog\", \"test\")\n",
            "data = pd.read_csv(Path(DATA_DIR, \"dialogues_test.txt\"),  delimiter = \"\\n\", names = [\"dialogues\"])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "def seputterances(row):\n",
            "    try:\n",
            "        row = row.split(\"__eou__\")\n",
            "        row = row[:-1]\n",
            "        return row\n",
            "    except:\n",
            "        return row\n",
            "\n",
            "data[\"dialogues\"] = data[\"dialogues\"].apply(seputterances)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/raid/cs18resch11003/anaconda3/envs/dialogpt/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                  "  from .autonotebook import tqdm as notebook_tqdm\n"
               ]
            }
         ],
         "source": [
            "from transformers import AutoTokenizer\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [],
         "source": [
            "num_context = 3\n",
            "\n",
            "utterance = []\n",
            "history = []\n",
            "\n",
            "for i in data.index:\n",
            "    row = data[\"dialogues\"][i]\n",
            "    for idx  in range(len(row)):\n",
            "        if idx >= 3:\n",
            "            utterance.append(row[idx])\n",
            "            counter = 1\n",
            "            _history = \"\"\n",
            "            \n",
            "            for k in range(idx-3, idx, 1):\n",
            "                if counter <= num_context:\n",
            "                    _history = _history + row[k]\n",
            "                    counter +=1\n",
            "                else:\n",
            "                    break\n",
            "                _history = _history + tokenizer.eos_token\n",
            "            history.append(_history)\n",
            "\n",
            "        elif idx!=0 and idx<3:\n",
            "            utterance.append(row[idx])\n",
            "            _history = \"\"\n",
            "            for k in range(idx):\n",
            "                _history = _history + row[k]\n",
            "                _history = _history + tokenizer.eos_token\n",
            "            history.append(_history)\n",
            "        else:\n",
            "            continue"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[\"I'll be willing to come and talk about the financing of our imports . \",\n",
                     " ' It can be solved by drawing a draft on us at 90 days sight . ',\n",
                     " ' What about a draft at 120 days sight ? ',\n",
                     " ' All right . But we demand the draft be accepted by a bank acceptable to us . ',\n",
                     " \" A bank's acceptance will add to the costs of our imports . You can rest assured that we will accept the draft and honour it when it comes due . \",\n",
                     " ' Then we will be in a position to arrange for a loan from our bank . You know we exports would like to have our investments returned as early as possible . ',\n",
                     " ' I hope this transaction will pave the way for further business relations between our two countries . ',\n",
                     " ' So do I . ']"
                  ]
               },
               "execution_count": 9,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "data[\"dialogues\"].iloc[50]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 83,
         "metadata": {},
         "outputs": [],
         "source": [
            "tokenizer.pad_token = tokenizer.eos_token\n",
            "max_len = 64"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 85,
         "metadata": {},
         "outputs": [],
         "source": [
            "import torch\n",
            "\n",
            "input_ids = []\n",
            "\n",
            "attention_masks = []\n",
            "\n",
            "labels = []\n",
            "\n",
            "for i in range(len(utterance)):\n",
            "        \n",
            "    encoded_utterance = tokenizer.encode_plus(utterance[i].lower() + tokenizer.eos_token, max_length = max_len, padding= \"max_length\", truncation = True, return_tensors = \"pt\")\n",
            "    \n",
            "    encoded_history = tokenizer.encode_plus(history[i].lower(), max_length = max_len, truncation = True, padding= \"max_length\", return_tensors = \"pt\")\n",
            "\n",
            "    \n",
            "    ids = torch.cat([encoded_history[\"input_ids\"][0], encoded_utterance[\"input_ids\"][0]], dim=0).reshape(1,max_len*2)\n",
            "    mask = torch.cat([encoded_history[\"attention_mask\"][0], encoded_utterance[\"attention_mask\"][0]], dim=0).reshape(1,max_len*2)\n",
            "\n",
            "    _label = torch.tensor([1 if element != 50256 else -100 for element in encoded_utterance[\"input_ids\"][0]])\n",
            "\n",
            "    label = torch.cat([torch.full((max_len,), -100), _label], dim = 0).reshape(1, max_len*2)\n",
            "\n",
            "    input_ids.append(ids)\n",
            "    attention_masks.append(mask)\n",
            "    labels.append(label)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 86,
         "metadata": {},
         "outputs": [],
         "source": [
            "input_ids = torch.cat(input_ids, dim = 0)\n",
            "attention_masks = torch.cat(attention_masks, dim=0)\n",
            "labels = torch.cat(labels, dim=0)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 87,
         "metadata": {},
         "outputs": [],
         "source": [
            "from torch.utils.data import TensorDataset\n",
            "\n",
            "dataset = TensorDataset(input_ids, attention_masks, labels)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 88,
         "metadata": {},
         "outputs": [],
         "source": [
            "num_batch = 8"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 89,
         "metadata": {},
         "outputs": [],
         "source": [
            "from torch.utils.data import DataLoader, RandomSampler\n",
            "\n",
            "dataloader = DataLoader(\n",
            "            dataset,\n",
            "            sampler = RandomSampler(dataset),\n",
            "            batch_size = num_batch\n",
            "        )"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 90,
         "metadata": {},
         "outputs": [],
         "source": [
            "from transformers import AutoModelForCausalLM\n",
            "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 91,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[INFO]: Working on GPU: cuda\n"
               ]
            }
         ],
         "source": [
            "import random\n",
            "\n",
            "SEED  = 1234\n",
            "\n",
            "random.seed(SEED)\n",
            "torch.manual_seed(SEED)\n",
            "torch.backends.cudnn.derterministic = True\n",
            "\n",
            "if torch.cuda.is_available():\n",
            "    torch.cuda.manual_seed_all(SEED)\n",
            "    device = torch.device(\"cuda\")\n",
            "    print(f\"[INFO]: Working on GPU: {device}\")\n",
            "else:\n",
            "    print(\"[INFO]: No GPU is available, using CPU instead\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 96,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Training the model ...\n",
                  "\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "  0%|          | 0/28521 [00:05<?, ?it/s]"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "torch.Size([8, 128]) torch.Size([8, 128]) torch.Size([8, 128])\n",
                  "torch.Size([8, 128]) torch.Size([8, 128]) torch.Size([8, 128])\n",
                  "torch.Size([8, 128]) torch.Size([8, 128]) torch.Size([8, 128])\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "\n"
               ]
            },
            {
               "ename": "",
               "evalue": "",
               "output_type": "error",
               "traceback": [
                  "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
               ]
            }
         ],
         "source": [
            "\n",
            "\n",
            "from transformers import get_scheduler\n",
            "from tqdm.auto import tqdm\n",
            "\n",
            "num_epochs = 3\n",
            "\n",
            "mode = model.to(device)\n",
            "\n",
            "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
            "\n",
            "num_training_steps = int(num_epochs * len(dataloader))\n",
            "\n",
            "lr_scheduler = get_scheduler(\n",
            "    \"linear\",\n",
            "    optimizer=optimizer,\n",
            "    num_warmup_steps=0,\n",
            "    num_training_steps=num_training_steps,\n",
            ")\n",
            "\n",
            "\n",
            "### TRAINING\n",
            "print(f\"Training the model ...\\n\")\n",
            "model.train()\n",
            "progress_bar = tqdm(range(num_training_steps))\n",
            "\n",
            "for epoch in range(num_epochs):\n",
            "    for i, batch in enumerate(dataloader):\n",
            "        print(batch[0].shape, batch[1].shape, batch[2].shape)\n",
            "        break\n",
            "        # b_input_ids = batch[0].to(device)\n",
            "        # b_attn_mask = batch[1].to(device)\n",
            "        # labels = batch[2].to(device)\n",
            "\n",
            "        # inputs = {\"input_ids\": b_input_ids, \"attention_mask\": b_attn_mask}\n",
            "\n",
            "        # optimizer.zero_grad()\n",
            "        \n",
            "        # outputs = model(**inputs, labels = labels)\n",
            "        # loss = outputs.loss\n",
            "\n",
            "        # if i%100 == 0:\n",
            "        #     print(f\"Epoch: {epoch}, Batch: {i}, Loss: {loss}\")\n",
            "        \n",
            "\n",
            "        # loss.backward()\n",
            "        # optimizer.step()\n",
            "        # lr_scheduler.step()\n",
            "\n",
            "        # progress_bar.update(1)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3.7.13 ('dialogpt')",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.7.13"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "47a16a6705441b6dda92a294eb31436c27952dbca87d6ec649ac2e31022a0dfe"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
